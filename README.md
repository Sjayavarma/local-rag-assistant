# local-rag-assistant
 DRIVE LINK TO ACEES OTHER FILE -- https://drive.google.com/drive/folders/1lq5AoDNSy-k4Ni9FbNSKba9xmQ59v_Kp?usp=sharing
A fully offline AI assistant that answers questions from PDFs using a local RAG pipeline with Mistral 7B, LangChain, FAISS, and Streamlit â€” no cloud, no API, privacy-first.

# ğŸ§  Local RAG Assistant â€“ Offline PDF Question Answering

ğŸš€ A fully offline, privacy-safe AI assistant that answers questions from your PDFs using a Retrieval-Augmented Generation (RAG) pipeline with **Mistral 7B**, **FAISS**, and a clean **Streamlit UI**.

ğŸ”— GitHub Repo: [https://github.com/Sjayavarma/local-rag-assistant](https://github.com/Sjayavarma/local-rag-assistant)

---

## ğŸ“– Project Description

This project is a local, API-free AI assistant that enables you to ask questions from your own PDF documents â€” all without sending any data to the cloud. It uses a Retrieval-Augmented Generation (RAG) architecture that combines **smart document chunking**, **semantic search**, and **local language generation** using `llama.cpp` and Mistral 7B.

Built to overcome the restrictions of cloud tools like AWS and OpenAI (cost, API limits, data privacy), this system gives you full control over your documents and inference â€” ideal for academic papers, internal reports, resumes, legal documents, and more.

---

## ğŸ”„ RAG Pipeline Flow

1. **ğŸ“„ Upload PDF** â€“ Load your document into the app.
2. **âœ‚ï¸ Chunking** â€“ LangChain splits the document into clean, digestible sections.
3. **ğŸ§  Embedding** â€“ Sentence-transformers embed each chunk as vectors.
4. **âš¡ Retrieval** â€“ FAISS finds the most relevant chunks based on your query.
5. **ğŸ¤– Generation** â€“ Mistral 7B (via llama.cpp) generates answers with that context.

---

## ğŸ” Features

- ğŸ“‚ Upload PDF files through a Streamlit interface
- ğŸ“ Automatic text extraction and chunking
- ğŸ§  Semantic search using `sentence-transformers` + FAISS
- ğŸ¤– Offline answer generation using Mistral 7B + llama.cpp
- ğŸ” Zero cloud dependencies â€“ fully private and local
- ğŸ›ï¸ Modern UI built with Streamlit

---

## ğŸ§° Tech Stack

| Layer        | Tool/Library                    |
|--------------|---------------------------------|
| UI           | Streamlit                       |
| Chunking     | LangChain                       |
| Embeddings   | `sentence-transformers`         |
| Vector Store | FAISS                           |
| LLM Engine   | `llama.cpp` + Mistral 7B `.gguf`|
| PDF Parser   | PyPDF2                          |
| Language     | Python 3.10+                    |

---

## ğŸ“¦ Installation Guide

### 1. Clone the repository

```bash
git clone https://github.com/Sjayavarma/local-rag-assistant.git
cd local-rag-assistant
#############2. Install Python dependencies
pip install -r requirements.txt
############3. Download the Mistral model
Download .gguf file from Hugging Face:
ğŸ‘‰ https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF

Recommended file: mistral-7b-instruct-v0.1.Q4_K_M.gguf

Place it inside the models/ folder:

mkdir models
######### Place your GGUF file here
4. Build llama.cpp (Windows)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
cmake -B build -A x64 -DLLAMA_CURL=ON .
cmake --build build --config Release
Or use a precompiled binary if available.

#################â–¶ï¸ Run the App
streamlit run app.py
Then open your browser at: http://localhost:8501

ğŸ“¸ Screenshots
Upload PDF	Chat With Docs

âœ¨ What I Learned
 ------ ( How to orchestrate a RAG pipeline locally

Working with embeddings and FAISS indexing

Building Streamlit UIs with authentication and chat flow

Running large models locally via llama.cpp ) 

ğŸ›¡ï¸ License
This project is licensed under the MIT License.

ğŸ¤ Contribution
Found a bug? Have an idea?
Feel free to fork, submit PRs, or raise issues. Collaboration is welcome!

ğŸ”— Author
Built with â¤ï¸ by Jayavarma S
ğŸ“§ Contact: sjayavarmas@gmail.com




